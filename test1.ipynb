{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This is the ipynb where I explain all the steps I took in order to solve the proposed Alert Term Extraction task by Thomas Moser and Michael Heil. The actual working program that encapsulates everything into classes and writes a json file to disk for testing is included in the same folder, here I put all the reasoning behind the final solution along with the code cells."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from cryptography.fernet import Fernet\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import spacy\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First step is to call the API to get the required data for working.\n",
    "\n",
    "The access key for the API has been encrypted symmetrically to not leave the key in clear sight into the repository.\n",
    "\n",
    "the first cell decrypts the key from the provided encryption keypair in order to make the API calls."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "enckey = open(\"key.key\", \"rb\").read()\n",
    "f = Fernet(enckey)\n",
    "with open(\"secretkey.key\", \"rb\") as file:\n",
    "    # read the encrypted data\n",
    "    encrypted_data = file.read()\n",
    "# decrypt data\n",
    "decrypted_data = f.decrypt(encrypted_data).decode()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This cell populates the parameters for the API calls (in our case only the authkey)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "payload = {'key': decrypted_data}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here the calls are made and the obtained data is parsed from the original JSON into a Pandas DataFrame for easier data analysis and manipulation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Terms Downloaded!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    r = requests.get('https://services.prewave.ai/adminInterface/api/testQueryTerm', params=payload)\n",
    "    queryterms = json.dumps(r.json(), indent = 4)\n",
    "    print('Query Terms Downloaded!')\n",
    "\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(errh)\n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print(errc)\n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print(errt)\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print(err)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alerts Downloaded!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    r = requests.get('https://services.prewave.ai/adminInterface/api/testAlerts', params=payload)\n",
    "    alerts = json.dumps(r.json(), indent = 4)\n",
    "    print('Alerts Downloaded!')\n",
    "\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(errh)\n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print(errc)\n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print(errt)\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print(err)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "      id  target                          text language  keepOrder\n0    101       1                     IG Metall       de       True\n1    102       1                     IG Metall       en       True\n2    103       1  Industriegewerkschaft Metall       de      False\n3    201       2                  Arbeitsplatz       de       True\n4    202       2                 Arbeitsplätze       de       True\n5    203       2                           job       en       True\n6    204       2                          jobs       en       True\n7    301       3                     pollution       en       True\n8    302       3                    inquinante       it       True\n9    401       4                       lithium       en       True\n10   501       5                         close       en       True\n11   502       5                       closure       en       True\n12   503       5                       closing       en       True\n13   601       6                         Tesla       en       True\n14   602       6                         Tesla       de       True\n15   603       6                         Tesla       it       True\n16   604       6                         Tesla       es       True\n17   701       7                         Yuasa       en       True\n18   801       8           minimal involvement       en      False\n19   901       9                   coronavirus       en       True\n20   902       9                   coronavirus       es       True\n21   903       9                      covid-19       en       True\n22   904       9                      covid-19       es       True\n23  1001      10                        muerte       es       True\n24  1101      11                          fake       en       True\n25  1102      11                        faking       en       True\n26  1201      12                      dp world       en       True\n27  1202      12                      dp world       es       True\n28  1203      12                      dp world       de       True\n29  1204      12             Dubai Ports World       en       True",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>text</th>\n      <th>language</th>\n      <th>keepOrder</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>101</td>\n      <td>1</td>\n      <td>IG Metall</td>\n      <td>de</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>102</td>\n      <td>1</td>\n      <td>IG Metall</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>103</td>\n      <td>1</td>\n      <td>Industriegewerkschaft Metall</td>\n      <td>de</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>201</td>\n      <td>2</td>\n      <td>Arbeitsplatz</td>\n      <td>de</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>202</td>\n      <td>2</td>\n      <td>Arbeitsplätze</td>\n      <td>de</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>203</td>\n      <td>2</td>\n      <td>job</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>204</td>\n      <td>2</td>\n      <td>jobs</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>301</td>\n      <td>3</td>\n      <td>pollution</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>302</td>\n      <td>3</td>\n      <td>inquinante</td>\n      <td>it</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>401</td>\n      <td>4</td>\n      <td>lithium</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>501</td>\n      <td>5</td>\n      <td>close</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>502</td>\n      <td>5</td>\n      <td>closure</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>503</td>\n      <td>5</td>\n      <td>closing</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>601</td>\n      <td>6</td>\n      <td>Tesla</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>602</td>\n      <td>6</td>\n      <td>Tesla</td>\n      <td>de</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>603</td>\n      <td>6</td>\n      <td>Tesla</td>\n      <td>it</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>604</td>\n      <td>6</td>\n      <td>Tesla</td>\n      <td>es</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>701</td>\n      <td>7</td>\n      <td>Yuasa</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>801</td>\n      <td>8</td>\n      <td>minimal involvement</td>\n      <td>en</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>901</td>\n      <td>9</td>\n      <td>coronavirus</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>902</td>\n      <td>9</td>\n      <td>coronavirus</td>\n      <td>es</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>903</td>\n      <td>9</td>\n      <td>covid-19</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>904</td>\n      <td>9</td>\n      <td>covid-19</td>\n      <td>es</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1001</td>\n      <td>10</td>\n      <td>muerte</td>\n      <td>es</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1101</td>\n      <td>11</td>\n      <td>fake</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1102</td>\n      <td>11</td>\n      <td>faking</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1201</td>\n      <td>12</td>\n      <td>dp world</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1202</td>\n      <td>12</td>\n      <td>dp world</td>\n      <td>es</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1203</td>\n      <td>12</td>\n      <td>dp world</td>\n      <td>de</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1204</td>\n      <td>12</td>\n      <td>Dubai Ports World</td>\n      <td>en</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfquery = pd.read_json(queryterms)\n",
    "display(dfquery)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data above shows the query terms. Apart from having their unique id, the terms are grouped together by target, that means that text with the same target number is semantically to be considered about the same entity.\n",
    "\n",
    "Data varies apart from the text itself on two dimensions: language and order, that means that compounds (word composed of more than one stem) with the keepOrder flag True should appear close one to the other, while where the flag is false the terms could be shifted in position or have other words inbetween (e.g. minimal involvment could appear as 'the involvment is minimal' and still need to be flagged)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "                id  \\\n0    ui75uz5z4hreg   \n1   u5rgtgz54bz4rr   \n2     i76u5zvferee   \n3  tz65hg4g5zht4gr   \n4     nuzbtju4654r   \n\n                                                                                                                                                                                                  contents  \\\n0  [{'text': 'Primo que honda los giles qué opinan de política pero todavía viven con los padres? Hasta que no vivan solos y paguen sus impuestos cierren los panes y no sequen el huevo', 'type': 'tex...   \n1                                                 [{'text': 'Tata Motors’ JLR extends COVID-19 production pause in UK https://t.co/a91RuzqsbB https://t.co/hkGjpJhtH7', 'type': 'text', 'language': 'en'}]   \n2  [{'text': '@ryanlangdon_ I don’t have a monologue but I also can’t visualize. So when I close my eyes all I see is black. It’s called aphantasia.  Check out Nicola Tesla. He could create worlds in...   \n3                                                                                                                          [{'text': 'the Rai’s a different breed bro', 'type': 'text', 'language': 'en'}]   \n4                       [{'text': 'Tesla self-driving traffic light and stop-sign interaction explained in leaked manual https://t.co/8gqNvkl4bS via @FredericLambert', 'type': 'text', 'language': 'en'}]   \n\n                              date inputType  \n0 2022-04-18 19:52:15.776000+00:00     tweet  \n1 2022-04-18 19:48:54.907000+00:00     tweet  \n2 2022-04-18 19:45:25.112000+00:00     tweet  \n3 2022-04-18 19:28:12.735000+00:00     tweet  \n4 2022-04-18 19:35:21.476000+00:00     tweet  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>contents</th>\n      <th>date</th>\n      <th>inputType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ui75uz5z4hreg</td>\n      <td>[{'text': 'Primo que honda los giles qué opinan de política pero todavía viven con los padres? Hasta que no vivan solos y paguen sus impuestos cierren los panes y no sequen el huevo', 'type': 'tex...</td>\n      <td>2022-04-18 19:52:15.776000+00:00</td>\n      <td>tweet</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>u5rgtgz54bz4rr</td>\n      <td>[{'text': 'Tata Motors’ JLR extends COVID-19 production pause in UK https://t.co/a91RuzqsbB https://t.co/hkGjpJhtH7', 'type': 'text', 'language': 'en'}]</td>\n      <td>2022-04-18 19:48:54.907000+00:00</td>\n      <td>tweet</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i76u5zvferee</td>\n      <td>[{'text': '@ryanlangdon_ I don’t have a monologue but I also can’t visualize. So when I close my eyes all I see is black. It’s called aphantasia.  Check out Nicola Tesla. He could create worlds in...</td>\n      <td>2022-04-18 19:45:25.112000+00:00</td>\n      <td>tweet</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tz65hg4g5zht4gr</td>\n      <td>[{'text': 'the Rai’s a different breed bro', 'type': 'text', 'language': 'en'}]</td>\n      <td>2022-04-18 19:28:12.735000+00:00</td>\n      <td>tweet</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>nuzbtju4654r</td>\n      <td>[{'text': 'Tesla self-driving traffic light and stop-sign interaction explained in leaked manual https://t.co/8gqNvkl4bS via @FredericLambert', 'type': 'text', 'language': 'en'}]</td>\n      <td>2022-04-18 19:35:21.476000+00:00</td>\n      <td>tweet</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfalerts = pd.read_json(alerts)\n",
    "with pd.option_context('display.max_colwidth', 200):\n",
    "    display(dfalerts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The alerts contain the ID of the alerts and texts in a JSON nested field with all the required attibutes for searching and matching terms: the text itself, the language is written in and the type of text. The date and input field can be skipped for this specific task.\n",
    "\n",
    "To better get the data needed for elaboration I am going to flatten the JSON text field to have all the data already in columns for extraction. Another issue is represented by the fact that we can have more than one text in the nested contents column, so we need to extract the text separately and put them in separate rows while sharing the same ID."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "   index               id                             date inputType  \\\n0  0      ui75uz5z4hreg   2022-04-18 19:52:15.776000+00:00  tweet      \n1  0      u5rgtgz54bz4rr  2022-04-18 19:48:54.907000+00:00  tweet      \n2  0      i76u5zvferee    2022-04-18 19:45:25.112000+00:00  tweet      \n3  0      tz65hg4g5zht4gr 2022-04-18 19:28:12.735000+00:00  tweet      \n4  0      nuzbtju4654r    2022-04-18 19:35:21.476000+00:00  tweet      \n\n                                                                                                                                                                                                                                                                                               text  \\\n0  Primo que honda los giles qué opinan de política pero todavía viven con los padres? Hasta que no vivan solos y paguen sus impuestos cierren los panes y no sequen el huevo                                                                                                                         \n1  Tata Motors’ JLR extends COVID-19 production pause in UK https://t.co/a91RuzqsbB https://t.co/hkGjpJhtH7                                                                                                                                                                                           \n2  @ryanlangdon_ I don’t have a monologue but I also can’t visualize. So when I close my eyes all I see is black. It’s called aphantasia.  Check out Nicola Tesla. He could create worlds in this mind that looked real. Sights, sounds, smells. He could see every working part of his inventions.   \n3  the Rai’s a different breed bro                                                                                                                                                                                                                                                                    \n4  Tesla self-driving traffic light and stop-sign interaction explained in leaked manual https://t.co/8gqNvkl4bS via @FredericLambert                                                                                                                                                                 \n\n   type language  \n0  text  es       \n1  text  en       \n2  text  en       \n3  text  en       \n4  text  en       ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>date</th>\n      <th>inputType</th>\n      <th>text</th>\n      <th>type</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>ui75uz5z4hreg</td>\n      <td>2022-04-18 19:52:15.776000+00:00</td>\n      <td>tweet</td>\n      <td>Primo que honda los giles qué opinan de política pero todavía viven con los padres? Hasta que no vivan solos y paguen sus impuestos cierren los panes y no sequen el huevo</td>\n      <td>text</td>\n      <td>es</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>u5rgtgz54bz4rr</td>\n      <td>2022-04-18 19:48:54.907000+00:00</td>\n      <td>tweet</td>\n      <td>Tata Motors’ JLR extends COVID-19 production pause in UK https://t.co/a91RuzqsbB https://t.co/hkGjpJhtH7</td>\n      <td>text</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>i76u5zvferee</td>\n      <td>2022-04-18 19:45:25.112000+00:00</td>\n      <td>tweet</td>\n      <td>@ryanlangdon_ I don’t have a monologue but I also can’t visualize. So when I close my eyes all I see is black. It’s called aphantasia.  Check out Nicola Tesla. He could create worlds in this mind that looked real. Sights, sounds, smells. He could see every working part of his inventions.</td>\n      <td>text</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>tz65hg4g5zht4gr</td>\n      <td>2022-04-18 19:28:12.735000+00:00</td>\n      <td>tweet</td>\n      <td>the Rai’s a different breed bro</td>\n      <td>text</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>nuzbtju4654r</td>\n      <td>2022-04-18 19:35:21.476000+00:00</td>\n      <td>tweet</td>\n      <td>Tesla self-driving traffic light and stop-sign interaction explained in leaked manual https://t.co/8gqNvkl4bS via @FredericLambert</td>\n      <td>text</td>\n      <td>en</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dfalerts['text'] =\n",
    "alertsflattened = []\n",
    "for i,row in dfalerts.iterrows():\n",
    "    flattenedcontentdf = pd.json_normalize(dfalerts['contents'][i])\n",
    "    flattenedcontentdf['id'] = row['id']\n",
    "    rowdf = pd.DataFrame(row).transpose()\n",
    "    mergeddf = pd.merge(rowdf,flattenedcontentdf, how=\"outer\", on=\"id\")\n",
    "    mergeddf = mergeddf.drop(columns='contents')\n",
    "    alertsflattened.append(mergeddf)\n",
    "dfalertsflattened = pd.concat(alertsflattened).reset_index()\n",
    "dfalertsflattened = dfalertsflattened[dfalertsflattened['text'].notna()]\n",
    "dfalertsflattened['language'] = dfalertsflattened['language'].apply(lambda x: unidecode(x))\n",
    "display(dfalertsflattened)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above code I sequentially:\n",
    "initialized a list to hold the flattened dataframes, iterated through the rows to extract the JSON inside the contents column, flattened it to a dataframe, transposed it in order to merge it with the remaining row columns, dropped the contents column and then hold the result into the initial list.\n",
    "At the end of the iteration process the dataframes are concatenated into a single dataframe with all the data needed for elaboration.\n",
    "\n",
    "In the above code sometimes the language code could be miswritten (I noticed in the data exploration that en is sometimes written as ên), a normalization is needed in order to have a consistent output for comparing and initializing the right language pipeline, that is what the line before displaying the dataframe does.\n",
    "\n",
    "The working process will be carried out through the library spaCy. I am going to inizialize the standard small trained language models that are available through the library, in order to take advantage of the pretrained tokenizers, the text will be fed into the right tokenizer according to the language attribute  extracted in the dataset. Note that because we have only 4 high-resources languages treated in our query terms I assume this approach is viable, in a bigger multilingual context it is better to recur to a rule-based method if resources are not available.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all, I extract the tuples of query terms to be fed to the language model for matching: spaCy has an internal PhraseMatcher object that matches query phrases with the content of a Document (the library structure created to hold texts). I am going to initialize a PhraseMatcher for every language with the related query terms according to their language attribute. The PhraseMatcher is going to hold the query term ID for backtracing the linked terms in the extraction phase, the terms themselves and the keepOrder attribute for creating a reversed entry in the matcher in case the attribute is False (e.g. in case of \"minimal involvement\" the phrase \"involvment minimal\" will be added too as a separate entry in the PhraseMatcher with the same label to track different phrase order.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def zipqueryterms(language:str):\n",
    "    return list(zip(list(dfquery[(dfquery['language'] == language)].id),list(dfquery[(dfquery['language'] == language)].text),list(dfquery[(dfquery['language'] == language)].keepOrder)))\n",
    "\n",
    "def createphrasematcher(languagemodel:spacy.Language,querytermslist:list):\n",
    "    matcherobject = PhraseMatcher(languagemodel.vocab, attr=\"LOWER\")\n",
    "    for id, term, order in querytermslist:\n",
    "        matcherobject.add(str(id), [languagemodel.make_doc(term.lower())])\n",
    "        if order == False:\n",
    "            matcherobject.add(str(id), [languagemodel.make_doc(\" \".join(term.lower().split(\" \")[::-1]))])\n",
    "    return matcherobject"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "eng = spacy.load('en_core_web_sm')\n",
    "spa = spacy.load('es_core_news_sm')\n",
    "deu = spacy.load('de_core_news_sm')\n",
    "ita = spacy.load('it_core_news_sm')\n",
    "\n",
    "engquery = zipqueryterms('en')\n",
    "spaquery = zipqueryterms('es')\n",
    "deuquery = zipqueryterms('de')\n",
    "itaquery = zipqueryterms('it')\n",
    "\n",
    "engmatcher = createphrasematcher(eng,engquery)\n",
    "spamatcher = createphrasematcher(spa,spaquery)\n",
    "deumatcher = createphrasematcher(deu,deuquery)\n",
    "itamatcher = createphrasematcher(ita,itaquery)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that the matcher objects are ready I am going to create the main algorithm: the advantage of using a language objects is that the texts from the dataset will be already tokenized, lowercased and stop words removal is just a matter of writing a couple of words.\n",
    "The matchers are set to match on the lowercased version of the words through the attribute \"LOWER\" that was set in their initialization.\n",
    "The text are going to be extracted with their ids and, according to their language attribute, a list of lowercased words where stop words and punctuations are removed will be returned and put into a document for feeding them to the phrasematcher. The Phrasematcher will return the matched text spans along with the query id of the matched phrase and this will be joined with the alert id in order to create a unique tuple of keys. The list of matches in the end is returned as a set to remove possible duplicates.\n",
    "\n",
    "Note that for every language that not falls in the 4 provided english is going to be assumed for tokenization and matching. I considered out of the task scope transliterating or even translating the sentences, as good neural models that are trained on the specific registry and domain of the target text is needed to reach good production quality. This approach make possible to get a marginally better result if entities are contained in an unknown language text, because if latin chars are used we will anyway obtain a correct match (e.g. the phrase \"dp world\")."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def tokenizetext(text:str, languagemodel:spacy.Language):\n",
    "    return [t.text.lower() for t in languagemodel(text) if (not t.is_stop and not t.is_punct)]\n",
    "\n",
    "def matchandreturn(tokenizedtext:list, phrasematcher:spacy.matcher.PhraseMatcher, tokenizedtextid:str, languagemodel:spacy.Language):\n",
    "    matcheslist = []\n",
    "    matches = phrasematcher(languagemodel(\" \".join(tokenizedtext)), as_spans=True)\n",
    "    for match in matches:\n",
    "        matcheslist.append((tokenizedtextid,match.label_,match.text))\n",
    "    return set(matcheslist)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function and the subsequent loop contain the business logic of the model, they extract every row and run the abovementioned algorithm returning the final match list, with empty sets excluded and sets with more than one match unnested in order to build easily the final dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def tokenizematchandappend(row:pd.Series, phrasematcher:spacy.matcher.PhraseMatcher, languagemodel:spacy.Language, finaldataset:list):\n",
    "    tokensentence = tokenizetext(row.text,languagemodel)\n",
    "    matchesset = matchandreturn(tokensentence,phrasematcher,row.id,languagemodel)\n",
    "    for matchset in matchesset:\n",
    "        finaldataset.append(matchset)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "matchesdataset = []\n",
    "for i,row in dfalertsflattened.iterrows():\n",
    "    if row.language == 'es':\n",
    "        tokenizematchandappend(row, spamatcher, spa, matchesdataset)\n",
    "    elif row.language == 'de':\n",
    "        tokenizematchandappend(row, deumatcher, deu, matchesdataset)\n",
    "    elif row.language == 'it':\n",
    "        tokenizematchandappend(row, itamatcher, ita, matchesdataset)\n",
    "    else:\n",
    "        tokenizematchandappend(row, engmatcher, eng, matchesdataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Last step is to put the data in a dataframe and dump it as a JSON to ensure readability and use an universal format that is easy to manipulate as an output.\n",
    "The format is {index:{alertid:value, queryid:value, text:value}}."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "'{\"0\":{\"alertid\":\"u5rgtgz54bz4rr\",\"queryid\":\"903\",\"text\":\"covid-19\"},\"1\":{\"alertid\":\"i76u5zvferee\",\"queryid\":\"501\",\"text\":\"close\"},\"2\":{\"alertid\":\"i76u5zvferee\",\"queryid\":\"601\",\"text\":\"tesla\"},\"3\":{\"alertid\":\"nuzbtju4654r\",\"queryid\":\"601\",\"text\":\"tesla\"}}'"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchesdf = pd.DataFrame(matchesdataset,columns=['alertid','queryid','text'])\n",
    "matchesdf.to_json(orient='index',force_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I leave this test code cell here: I used it to show if the stop words removal is effectively working and if the keepOrder flag equals False mechanism is working correctly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(102, 'IG Metall', True), (203, 'job', True), (204, 'jobs', True), (301, 'pollution', True), (401, 'lithium', True), (501, 'close', True), (502, 'closure', True), (503, 'closing', True), (601, 'Tesla', True), (701, 'Yuasa', True), (801, 'minimal involvement', False), (901, 'coronavirus', True), (903, 'covid-19', True), (1101, 'fake', True), (1102, 'faking', True), (1201, 'dp world', True), (1204, 'Dubai Ports World', True)]\n",
      "[]\n",
      "involvement minimal 801\n"
     ]
    }
   ],
   "source": [
    "teststring = 'the involvement is minimal looking for a big deal.'\n",
    "print(engquery)\n",
    "#secmatches = engmatcher(eng(dfalertsflattened.loc[3].text))\n",
    "secmatches = engmatcher(eng(teststring), as_spans=True)\n",
    "#for match in secmatches:\n",
    "print(secmatches)\n",
    "secmatches2 = engmatcher(eng(\" \".join(tokenizetext(teststring,eng))), as_spans=True)\n",
    "print(secmatches2[0], secmatches2[0].label_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a text output with the query and alerts datasets and the found match with ids and full text for testing purposes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY TERMS\n",
      "      id                          text language\n",
      "0   101   IG Metall                     de     \n",
      "1   102   IG Metall                     en     \n",
      "2   103   Industriegewerkschaft Metall  de     \n",
      "3   201   Arbeitsplatz                  de     \n",
      "4   202   Arbeitsplätze                 de     \n",
      "5   203   job                           en     \n",
      "6   204   jobs                          en     \n",
      "7   301   pollution                     en     \n",
      "8   302   inquinante                    it     \n",
      "9   401   lithium                       en     \n",
      "10  501   close                         en     \n",
      "11  502   closure                       en     \n",
      "12  503   closing                       en     \n",
      "13  601   Tesla                         en     \n",
      "14  602   Tesla                         de     \n",
      "15  603   Tesla                         it     \n",
      "16  604   Tesla                         es     \n",
      "17  701   Yuasa                         en     \n",
      "18  801   minimal involvement           en     \n",
      "19  901   coronavirus                   en     \n",
      "20  902   coronavirus                   es     \n",
      "21  903   covid-19                      en     \n",
      "22  904   covid-19                      es     \n",
      "23  1001  muerte                        es     \n",
      "24  1101  fake                          en     \n",
      "25  1102  faking                        en     \n",
      "26  1201  dp world                      en     \n",
      "27  1202  dp world                      es     \n",
      "28  1203  dp world                      de     \n",
      "29  1204  Dubai Ports World             en     \n",
      "------------------\n",
      "ALERTS\n",
      "                id  \\\n",
      "0  ui75uz5z4hreg     \n",
      "1  u5rgtgz54bz4rr    \n",
      "2  i76u5zvferee      \n",
      "3  tz65hg4g5zht4gr   \n",
      "4  nuzbtju4654r      \n",
      "\n",
      "                                                                                                                                                                                                                                                                                               text  \\\n",
      "0  Primo que honda los giles qué opinan de política pero todavía viven con los padres? Hasta que no vivan solos y paguen sus impuestos cierren los panes y no sequen el huevo                                                                                                                         \n",
      "1  Tata Motors’ JLR extends COVID-19 production pause in UK https://t.co/a91RuzqsbB https://t.co/hkGjpJhtH7                                                                                                                                                                                           \n",
      "2  @ryanlangdon_ I don’t have a monologue but I also can’t visualize. So when I close my eyes all I see is black. It’s called aphantasia.  Check out Nicola Tesla. He could create worlds in this mind that looked real. Sights, sounds, smells. He could see every working part of his inventions.   \n",
      "3  the Rai’s a different breed bro                                                                                                                                                                                                                                                                    \n",
      "4  Tesla self-driving traffic light and stop-sign interaction explained in leaked manual https://t.co/8gqNvkl4bS via @FredericLambert                                                                                                                                                                 \n",
      "\n",
      "  language  \n",
      "0  es       \n",
      "1  en       \n",
      "2  en       \n",
      "3  en       \n",
      "4  en       \n",
      "------------------\n",
      "MATCHES\n",
      "------------------\n",
      "ALERT ID\n",
      "u5rgtgz54bz4rr\n",
      "ALERT TEXT\n",
      "1    Tata Motors’ JLR extends COVID-19 production pause in UK https://t.co/a91RuzqsbB https://t.co/hkGjpJhtH7\n",
      "Name: text, dtype: object\n",
      "QUERY TERM ID\n",
      "903\n",
      "QUERY TERM MATCHED\n",
      "covid-19\n",
      "------------------\n",
      "ALERT ID\n",
      "i76u5zvferee\n",
      "ALERT TEXT\n",
      "2    @ryanlangdon_ I don’t have a monologue but I also can’t visualize. So when I close my eyes all I see is black. It’s called aphantasia.  Check out Nicola Tesla. He could create worlds in this mind that looked real. Sights, sounds, smells. He could see every working part of his inventions.\n",
      "Name: text, dtype: object\n",
      "QUERY TERM ID\n",
      "501\n",
      "QUERY TERM MATCHED\n",
      "close\n",
      "------------------\n",
      "ALERT ID\n",
      "i76u5zvferee\n",
      "ALERT TEXT\n",
      "2    @ryanlangdon_ I don’t have a monologue but I also can’t visualize. So when I close my eyes all I see is black. It’s called aphantasia.  Check out Nicola Tesla. He could create worlds in this mind that looked real. Sights, sounds, smells. He could see every working part of his inventions.\n",
      "Name: text, dtype: object\n",
      "QUERY TERM ID\n",
      "601\n",
      "QUERY TERM MATCHED\n",
      "tesla\n",
      "------------------\n",
      "ALERT ID\n",
      "nuzbtju4654r\n",
      "ALERT TEXT\n",
      "4    Tesla self-driving traffic light and stop-sign interaction explained in leaked manual https://t.co/8gqNvkl4bS via @FredericLambert\n",
      "Name: text, dtype: object\n",
      "QUERY TERM ID\n",
      "601\n",
      "QUERY TERM MATCHED\n",
      "tesla\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 0\n",
    "print(\"QUERY TERMS\")\n",
    "print(dfquery[[\"id\",\"text\",\"language\"]])\n",
    "print(\"------------------\")\n",
    "print(\"ALERTS\")\n",
    "print(dfalertsflattened[[\"id\", \"text\",\"language\"]])\n",
    "print(\"------------------\")\n",
    "print(\"MATCHES\")\n",
    "print(\"------------------\")\n",
    "for i,row in matchesdf.iterrows():\n",
    "    print(\"ALERT ID\")\n",
    "    print(row.alertid)\n",
    "    print(\"ALERT TEXT\")\n",
    "    print(dfalertsflattened[(dfalertsflattened['id']==row.alertid)].text)\n",
    "    print(\"QUERY TERM ID\")\n",
    "    print(row.queryid)\n",
    "    print(\"QUERY TERM MATCHED\")\n",
    "    print(row.text)\n",
    "    print(\"------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}